{
  "working_json_outline": {
    "objectives": [
      "KG-anchored, explainable regulatory QA with explicit policy-graph reasoning",
      "Reproducible eval with groundedness-first metrics and evaluator failure-mode checks"
    ],

    "models": {
      "training_strategy": {
        "instruction_tuning": "QLoRA or full-finetune on regulatory corpora with provenance",
        "alignment": "DPO/RLAIF with citation-first prompts",
        "long_context": "routing to long-context reader for statutes and guidance"
      },
      "candidates": {
        "general_llm_baselines": [
          "Llama-3.1 Instruct (8B/70B) — baseline",
          "Mistral Large class — baseline",
          "Claude-class or GPT-class API models — yardstick only"
        ],
        "legal_domain_baselines": [
          "Org-tuned Llama-3.1 legal variant (fine-tuned on legal/regulatory corpora)",
          "Legal-BERT family encoders for retrieval and classification",
          "LED/Longformer-family encoder-decoder for long-document summarization/reranking"
        ],
        "selection_criteria": [
          "Groundedness under citation constraints",
          "Latency and cost per compliant answer",
          "Explanation fidelity vs KG evidence"
        ]
      }
    },

    "kg_construction": {
      "ontology": "EAR ontology with extensions for policy logic",
      "policy_graph": {
        "node_types": [
          "Definition",
          "Entity",
          "Obligation",
          "Prohibition",
          "Permission",
          "Exception",
          "ConditionPrerequisite",
          "ProcedureStep",
          "EffectiveDate",
          "CrossReference"
        ],
        "edge_types": [
          "applies_to(Entity)",
          "defined_in(Definition → Term)",
          "requires(Obligation/ProcedureStep → ConditionPrerequisite)",
          "excepts(Obligation/Prohibition → Exception)",
          "permits(Permission → Action)",
          "supersedes(§ → §)",
          "effective_on(§ → EffectiveDate)",
          "refers_to(§ → CrossReference)"
        ],
        "constraints": {
          "shacl": [
            "Every Obligation must have at least one applies_to(Entity)",
            "Every Exception must point to at least one target rule",
            "Every ConditionPrerequisite must be boolean-evaluable"
          ],
          "temporal": "Queries resolved against applicable EffectiveDate windows"
        },
        "reasoning_templates": [
          "obligation_check(Entity, Context): walk requires → excepts → effective_on",
          "permission_with_exception(Entity, Action): permits minus applicable excepts",
          "multi_step_compliance(Entity, Procedure): topological order over ProcedureStep.requires"
        ],
        "evidence_provenance": "Each triple links to regulation span IDs and document hashes"
      }
    },

    "retrieval_and_reasoning": {
      "hybrid_retrieval": [
        "BM25 + dense encoders",
        "GraphRAG subgraph expansion from policy_graph",
        "SPARQL templates for obligation/exception chains"
      ],
      "answering": {
        "constraint_checking": "Validate draft answers against KG constraints before emit",
        "explanation_payload": [
          "cited_text_spans",
          "kg_paths_traced",
          "applicability_window",
          "exceptions_considered"
        ]
      }
    },

    "evaluation_and_explainability": {
      "benchmarks": {
        "task_sets": [
          "Obligation/exception resolution",
          "Applicability and effective date checks",
          "Multi-step compliance procedures",
          "Unanswerable/insufficient-evidence detection"
        ],
        "groundedness_metrics": [
          "Citation Precision/Recall (span-level)",
          "Evidence F1 (document+span)",
          "Attribution Support Score (does evidence entail the claim)",
          "KG-Constraint Consistency Rate",
          "Faithfulness Score (statement-to-evidence)",
          "Answerability Accuracy (reject when evidence absent)"
        ],
        "quality_metrics": [
          "Exact/Soft match on final decision",
          "Step consistency for multi-step proofs",
          "Latency and cost per answer"
        ],
        "evaluator_failure_modes": [
          "Ambiguity sensitivity: performance with intentionally ambiguous prompts",
          "Version drift: performance when statutes differ by date",
          "Adversarial citations: model penalized if citing irrelevant spans",
          "Evaluator leakage checks: holdout splits by section/version",
          "Rater variance audits: inter-rater agreement and bias checks"
        ]
      },
      "reporting": {
        "per_answer_artifacts": [
          "ranked_evidence_spans",
          "kg_path_visualization_ref",
          "validation_results(shacl, temporal)",
          "decision_trace with step rationales"
        ],
        "run_artifacts": [
          "model_card with training data provenance",
          "eval_manifest with seeds and corpora hashes",
          "audit_log of prompts, versions, and config"
        ]
      }
    },

    "acceptance_criteria": [
      "Groundedness: Evidence F1 ≥ target and KG-Consistency ≥ target on holdout",
      "Citation precision ≥ target with zero known bad citations on red-team set",
      "Evaluator failure-mode suites pass thresholds",
      "Per-answer artifacts include citations and KG paths",
      "Temporal validity checks enforced for effective dates"
    ]
  }
}
