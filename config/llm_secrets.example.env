# Copy this file to config/llm_secrets.env and fill in your keys.
# Values in this file are OPTIONAL; environment variables and the
# Windows Credential Manager (via keyring) remain the primary sources.
# Any value set in the environment takes precedence over this file.

# Default provider: groq | nvidia_nim
LLM_PROVIDER=groq

# NVIDIA NIM (OpenAI-compatible endpoint)
# BASE URL should include the /v1 prefix if required by your deployment.
NVIDIA_NIM_API_KEY=
NVIDIA_NIM_BASE_URL=
# Model IDs vary by deployment; set this explicitly for your NIM endpoint.
NVIDIA_NIM_MODEL=

# Groq (https://console.groq.com/docs)
GROQ_API_KEY=
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=llama-3.3-70b-versatile

# Optional per-run safety rails
# LLM_MAX_CALLS sets a soft budget across providers (blank disables)
LLM_MAX_CALLS=
# Provider-specific budgets (blank disables)
LLM_NVIDIA_NIM_MAX_CALLS=
LLM_GROQ_MAX_CALLS=

# Toggle remote LLM usage (default: disabled to keep CI/offline runs deterministic)
# Set to 1 to allow networked LLM calls.
EARCRAWLER_ENABLE_REMOTE_LLM=0
